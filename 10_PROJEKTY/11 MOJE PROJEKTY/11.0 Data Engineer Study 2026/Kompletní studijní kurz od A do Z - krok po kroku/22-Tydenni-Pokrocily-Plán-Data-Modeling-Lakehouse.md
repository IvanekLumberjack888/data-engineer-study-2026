# üìä 22-T√ùDENN√ç POKROƒåIL√ù STUDIJN√ç PL√ÅN: Data Modeling & Lakehouse

**C√≠l:** Od fundament≈Ø k Senior Junior Data Engineer s pochopen√≠m datov√©ho modelov√°n√≠, Lakehouse architecture a enterprise workflow

**ƒåasov√Ω r√°mec:** Listopad 2025 - B≈ôezen 2026 (161 dn√≠ = ~23 t√Ωdn≈Ø)

**V√Ωstup:**
- ‚úÖ 3 portfoliov√© projekty (Local ETL ‚Üí Cloud ‚Üí Real-time)
- ‚úÖ Porozumƒõn√≠ Star Schema, Data Vault, Lakehouse
- ‚úÖ Databricks/Delta Lake hands-on zku≈°enost
- ‚úÖ Datov√° kvalita a governance
- ‚úÖ AI-driven features (Natural Language, sentiment analysis)

---

## üóìÔ∏è 22-T√ùDENN√ç PL√ÅN (Detailn√≠ rozvrh)

### F√ÅZE 1: FUNDAMENTY A ARCHITEKTURA (T√Ωdny 1-2)

#### T√Ωden 1: √övod do Data Engineeringu a Modern√≠ch Architektur

**Hlavn√≠ t√©mata:**
- Co je Data Engineering? Role, odpovƒõdnosti, kari√©rn√≠ cesty
- Evoluce: Data Warehouse ‚Üí Data Lake ‚Üí Lakehouse
- Kl√≠ƒçov√© pojmy: ETL vs ELT, Batch vs Streaming

**Uƒçebn√≠ c√≠le:**
- [ ] Rozum√≠m rozd√≠lu mezi DW, DL, Lakehouse
- [ ] V√≠m, co dƒõl√° Data Engineer vs Data Analyst vs Data Scientist
- [ ] Zn√°m z√°kladn√≠ workflow: Source ‚Üí Raw ‚Üí Process ‚Üí Insight

**Praktika:**
```python
# Diagram: Datov√© toky v Lakehouse
# Source (API, Database) 
#   ‚Üì
# Raw (Bronze)
#   ‚Üì
# Clean (Silver)
#   ‚Üì
# Business Ready (Gold)
#   ‚Üì
# BI/AI Applications
```

**Obsidian:**
- `Learn/01-Data-Engineering-Fundamentals.md`
- Zapi≈° si: architektury, role, workflow

**GitHub Task:**
```bash
git add docs/01-Lakehouse-Architecture.md
git commit -m "üìä Understanding Lakehouse architecture"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Data modelov√°n√≠ - jak strukturovat data?

---

#### T√Ωden 2: Z√°kladn√≠ Datov√© Modelov√°n√≠ - Logick√© a Fyzick√© Modely

**Hlavn√≠ t√©mata:**
- Logick√Ω model vs Fyzick√Ω model
- Modelov√°n√≠ pro DW/OLAP syst√©my
- Fakta (Measures), Dimenze (Context)
- Popis, kardinalita, hierarchie

**Uƒçebn√≠ c√≠le:**
- [ ] Rozum√≠m rozd√≠lu mezi logick√Ωm a fyzick√Ωm modelem
- [ ] V√≠m, co je fakt a co je dimenze
- [ ] Zn√°m typy fakt≈Ø: additive, semi-additive, non-additive
- [ ] V√≠m, co je kardinalita a hierarchie dimenz√≠

**Praktika:**
```python
# P≈ô√≠klad: Tabulka objedn√°vek (Sales Pipeline)

# FACT TABLE - Prodeje
SELECT order_id, date_id, customer_id, product_id, 
       quantity, revenue, cost
FROM fact_sales;

# DIMENSION TABLE - ƒåas
SELECT date_id, date, month, quarter, year
FROM dim_date;

# DIMENSION TABLE - Produkt
SELECT product_id, name, category, supplier_id
FROM dim_product;

# DIMENSION TABLE - Z√°kazn√≠k
SELECT customer_id, name, country, segment
FROM dim_customer;
```

**Obsidian:**
- `Learn/02-Logical-Physical-Models.md`
- Zapi≈° si: fact types, dimensions, hierarchy

**GitHub Task:**
```bash
git add projects/01-etl-local/data_model.md
git commit -m "üìã Creating dimensional model for sales"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Star Schema a normalizace!

---

### F√ÅZE 2: DIMENZION√ÅLN√ç MODELOV√ÅN√ç (T√Ωdny 3-7)

#### T√Ωden 3: Star Schema - Nejjednodu≈°≈°√≠ P≈ô√≠stup

**Hlavn√≠ t√©mata:**
- Star Schema principy
- Centr√°ln√≠ fact tabulka obklopen√° dimenzemi
- Denormalizace pro rychlost
- Index strategie

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m Star Schema pattern
- [ ] V√≠m, kdy pou≈æ√≠t Star vs Snowflake
- [ ] Rozum√≠m denormalizaci v praxi
- [ ] V√≠m, jak designovat fact tabulku bez redundance

**Praktika:**
```sql
-- Star Schema: Fact + 4 Dimensions

-- FACT TABLE (mal√©, jen obchodn√≠ metriky)
CREATE TABLE fact_sales (
    order_id INT PRIMARY KEY,
    date_id INT,           -- FK do dim_date
    customer_id INT,       -- FK do dim_customer
    product_id INT,        -- FK do dim_product
    quantity INT,
    revenue DECIMAL(10,2),
    cost DECIMAL(10,2)
);

-- DIMENSION TABLES (velk√©, atributy)
CREATE TABLE dim_date (
    date_id INT PRIMARY KEY,
    date DATE,
    month VARCHAR(10),
    quarter INT,
    year INT
);

CREATE TABLE dim_customer (
    customer_id INT PRIMARY KEY,
    name VARCHAR(100),
    country VARCHAR(50),
    segment VARCHAR(20),
    registration_date DATE
);

-- Query: Tr≈æby za kategorie produkt≈Ø
SELECT 
    dp.category,
    SUM(fs.revenue) as total_revenue,
    COUNT(*) as order_count
FROM fact_sales fs
INNER JOIN dim_date dd ON fs.date_id = dd.date_id
INNER JOIN dim_product dp ON fs.product_id = dp.product_id
WHERE dd.year = 2025
GROUP BY dp.category
ORDER BY total_revenue DESC;
```

**Obsidian:**
- `Learn/03-Star-Schema.md`
- Zapi≈° si: design pattern, query examples

**GitHub Task:**
```bash
git add projects/01-etl-local/star_schema.sql
git commit -m "‚≠ê Implementing Star Schema for sales"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Snowflake Schema - komplexnƒõj≈°√≠ modely

---

#### T√Ωden 4: Snowflake Schema - Normalizovan√Ω P≈ô√≠stup

**Hlavn√≠ t√©mata:**
- Snowflake Schema: normalizovan√© dimenze
- Trade-off: m√©nƒõ redundance vs v√≠ce join≈Ø
- Kdy volit Snowflake vs Star
- Hierarchie dimenz√≠ v Snowflake

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m Snowflake Schema pattern
- [ ] V√≠m, kdy je Snowflake lep≈°√≠ ne≈æ Star
- [ ] Rozum√≠m hierarchi√≠m v Snowflake
- [ ] V√≠m o performance trade-offech

**Praktika:**
```sql
-- Snowflake Schema: Normalizovan√© Dimenze

-- FACT TABLE (stejn√© jako Star)
CREATE TABLE fact_sales (
    order_id INT PRIMARY KEY,
    date_id INT,
    customer_id INT,
    product_id INT,
    quantity INT,
    revenue DECIMAL(10,2)
);

-- DIMENSION: Produkt (nyn√≠ normalizovan√°!)
CREATE TABLE dim_product (
    product_id INT PRIMARY KEY,
    name VARCHAR(100),
    category_id INT  -- FK do dim_category
);

CREATE TABLE dim_category (
    category_id INT PRIMARY KEY,
    category_name VARCHAR(50),
    subcategory_id INT  -- FK do dim_subcategory
);

CREATE TABLE dim_subcategory (
    subcategory_id INT PRIMARY KEY,
    subcategory_name VARCHAR(50)
);

-- Query: Tr≈æby za podkategorie
-- POZOR: V√≠ce join≈Ø! Ale m√©nƒõ redundance
SELECT 
    ds.subcategory_name,
    dc.category_name,
    SUM(fs.revenue) as total_revenue
FROM fact_sales fs
INNER JOIN dim_product dp ON fs.product_id = dp.product_id
INNER JOIN dim_category dc ON dp.category_id = dc.category_id
INNER JOIN dim_subcategory ds ON dc.subcategory_id = ds.subcategory_id
GROUP BY ds.subcategory_name, dc.category_name;
```

**Obsidian:**
- `Learn/04-Snowflake-Schema.md`
- Porovn√°n√≠: Star vs Snowflake

**GitHub Task:**
```bash
git add projects/01-etl-local/snowflake_schema.sql
git commit -m "‚ùÑÔ∏è Comparing Star vs Snowflake Schema"
```

**P≈ô√≠≈°t√≠ t√Ωden:** SCD - jak se dimenze mƒõn√≠ v ƒçase?

---

#### T√Ωden 5: Slowly Changing Dimensions (SCD) Type 1 & 2

**Hlavn√≠ t√©mata:**
- SCD Type 0: Nech√°nout jak je (historii nehl√≠d√°me)
- SCD Type 1: P≈ôepsat (≈æ√°dn√° historie)
- SCD Type 2: Nov√° ≈ô√°dka (kompl√©tn√≠ historie)
- SCD Type 3: Extra sloupce (limitovan√° historie)

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m v≈°echny SCD typy
- [ ] V√≠m, kdy pou≈æ√≠t Type 1 vs Type 2 vs Type 3
- [ ] Um√≠m implementovat SCD v Pythonu/SQL
- [ ] Rozum√≠m surrogate keys pro Type 2

**Praktika:**
```python
# P≈ô√≠klad: Dimenze Z√°kazn√≠k - SCD Type 2

# SCD Type 2: Nov√° ≈ô√°dka s histori√≠
# Z√°kazn√≠k zmƒõn√≠ bydli≈°tƒõ ‚Üí nov√° ≈ô√°dka!

# Origin√°ln√≠ data:
customer_id=1, name="Jan", country="CZ", effective_date="2023-01-01", end_date=NULL

# Z√°kazn√≠k se stƒõhuje do Nƒõmecka (2025-06-15)
# Nov√° ≈ô√°dka:
customer_id=1, name="Jan", country="CZ", effective_date="2023-01-01", end_date="2025-06-15"
customer_id=1, name="Jan", country="DE", effective_date="2025-06-16", end_date=NULL

# V√Ωhoda: Historick√© anal√Ωzy funguj√≠!
# "Kolik ƒåech≈Ø n√°m odjelo do zahraniƒç√≠?"
```

```sql
-- Implementace SCD Type 2 v SQL (MERGE pattern)
MERGE INTO dim_customer dc
USING (SELECT * FROM staging_customer) sc
ON dc.customer_id = sc.customer_id AND dc.is_current = 1
WHEN MATCHED AND dc.country != sc.country THEN
    UPDATE SET is_current = 0, end_date = CURRENT_DATE
WHEN NOT MATCHED THEN
    INSERT (customer_id, name, country, effective_date, end_date, is_current)
    VALUES (sc.customer_id, sc.name, sc.country, CURRENT_DATE, NULL, 1);
```

**Obsidian:**
- `Learn/05-SCD-Types.md`
- Praktick√© p≈ô√≠klady, diagramy

**GitHub Task:**
```bash
git add projects/01-etl-local/scd_implementation.py
git commit -m "üìÖ Implementing SCD Type 2 for slowly changing dimensions"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Speci√°ln√≠ dimenze - Role-Playing, Junk, Degenerate

---

#### T√Ωden 6: Speci√°ln√≠ Dimenze a Pokroƒçil√© Koncepty

**Hlavn√≠ t√©mata:**
- Degenerate Dimension (kl√≠ƒç bez tabulky)
- Junk/Garbage Dimension (n√≠zko-kardinalitn√≠ vlajky)
- Role-Playing Dimension (jedna tabulka, v√≠ce rol√≠)
- Fast Changing Dimension (mini-dimenze)

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m speci√°ln√≠ dimenze patterny
- [ ] V√≠m, kdy je pou≈æ√≠t
- [ ] Rozum√≠m deskriptivn√≠m atribut≈Øm
- [ ] V√≠m, jak optimalizovat kardinalitu

**Praktika:**
```sql
-- P≈ô√≠klady:

-- 1. DEGENERATE DIMENSION
-- TransactionNo bez tabulky - existuje jen v faktu!
CREATE TABLE fact_transactions (
    transaction_no VARCHAR(20),  -- Degenerate DIM
    order_id INT,
    amount DECIMAL(10,2)
);

-- 2. JUNK DIMENSION
-- Kombinuje n√≠zko-kardinalitn√≠ vlajky
CREATE TABLE dim_order_flags (
    order_flag_id INT PRIMARY KEY,
    is_rush_order BIT,           -- 2 hodnoty: 0/1
    is_priority_customer BIT,    -- 2 hodnoty: 0/1
    payment_method VARCHAR(10)   -- 5 mo≈ænost√≠
    -- Kombinace: 2 √ó 2 √ó 5 = 20 ≈ô√°dk≈Ø maxim√°lnƒõ!
);

-- 3. ROLE-PLAYING DIMENSION
-- Stejn√° tabulka hraje v√≠ce rol√≠ (r≈Øzn√© datum)
-- Tabulka: dim_date
-- Fact table:
CREATE TABLE fact_orders (
    order_id INT,
    order_date_id INT,      -- FK do dim_date (Role 1: Kdy byla objednan√°)
    ship_date_id INT,       -- FK do dim_date (Role 2: Kdy byla odesl√°na)
    delivery_date_id INT,   -- FK do dim_date (Role 3: Kdy dorazila)
    amount DECIMAL(10,2)
);
```

**Obsidian:**
- `Learn/06-Special-Dimensions.md`
- Diagramy a use cases

**GitHub Task:**
```bash
git add projects/01-etl-local/special_dimensions.sql
git commit -m "üé≠ Implementing Role-Playing, Junk, Degenerate Dimensions"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Mini project - praktick√© modelov√°n√≠

---

#### T√Ωden 7: Mini Project - Datov√© Modelov√°n√≠ v Praxi

**Projekt:** Postavit dimenzion√°ln√≠ model pro E-commerce (Star Schema)

**Specifikace:**
```
Source data:
- Orders (order_id, customer_id, product_id, order_date, quantity, price)
- Customers (customer_id, name, email, country, signup_date)
- Products (product_id, name, category, price, supplier_id)
- Calendar (date, month, quarter, year)

V√Ωstup:
- fact_sales (order_id, date_id, customer_id, product_id, quantity, revenue, cost)
- dim_date (date_id, date, month, quarter, year)
- dim_customer (customer_id, name, email, country, segment, is_current)
- dim_product (product_id, name, category, supplier_id)
```

**Deliverable:**
- SQL script: CREATE TABLEs + indexy
- Dokumentace: Entity Relationship Diagram
- 5x sample queries

**GitHub Task:**
```bash
git add projects/Project-1-ETL-Local/ecommerce_model.sql
git add projects/Project-1-ETL-Local/README.md
git commit -m "üéØ E-commerce dimensional model complete"
```

**P≈ô√≠≈°t√≠ t√Ωden:** F√°ze 3 - Data Vault!

---

### F√ÅZE 3: DATA VAULT A MODERN√ç P≈ò√çSTUPY (T√Ωdny 8-9)

#### T√Ωden 8: Data Vault - Enterprise Approach

**Hlavn√≠ t√©mata:**
- Data Vault metodologie: Hub-Link-Satellite
- Proƒç Data Vault? (flexibility, auditovatelnost, skalabilita)
- Hub: jedineƒçn√© business kl√≠ƒçe
- Link: vztahy mezi hubem
- Satellite: atributy a historie

**Uƒçebn√≠ c√≠le:**
- [ ] Rozum√≠m Data Vault struktu≈ôe (Hub-Link-Sat)
- [ ] V√≠m, kdy volit Data Vault vs Dimensional
- [ ] Zn√°m v√Ωhody flexibilitu Vault
- [ ] Um√≠m navrhovat Hub tabulky

**Praktika:**
```sql
-- Data Vault: Hub-Link-Satellite Pattern

-- HUB: Jedineƒçn√© business kl√≠ƒçe
CREATE TABLE hub_customer (
    customer_key INT PRIMARY KEY SURROGATE,
    customer_id INT,              -- Business Key
    load_date TIMESTAMP,
    record_source VARCHAR(20)
);

-- HUB: Produkty
CREATE TABLE hub_product (
    product_key INT PRIMARY KEY SURROGATE,
    product_id INT,               -- Business Key
    load_date TIMESTAMP,
    record_source VARCHAR(20)
);

-- LINK: Vztahy (Customer koupit Product)
CREATE TABLE link_customer_product (
    link_key INT PRIMARY KEY SURROGATE,
    customer_key INT,             -- FK hub_customer
    product_key INT,              -- FK hub_product
    load_date TIMESTAMP,
    record_source VARCHAR(20)
);

-- SATELLITE: Atributy (zmƒõny v ƒçase)
CREATE TABLE sat_customer_details (
    customer_key INT,             -- FK hub_customer
    load_date TIMESTAMP,
    name VARCHAR(100),
    email VARCHAR(100),
    country VARCHAR(50),
    end_date TIMESTAMP
);

CREATE TABLE sat_product_details (
    product_key INT,              -- FK hub_product
    load_date TIMESTAMP,
    name VARCHAR(100),
    category VARCHAR(50),
    price DECIMAL(10,2),
    end_date TIMESTAMP
);

-- V√Ωhoda: Kdy≈æ se p≈ôid√° nov√Ω produkt:
-- Staƒç√≠ p≈ôidat ≈ô√°dku do hub_product a satelit
-- Nemus√≠≈° mƒõnit ostatn√≠ struktury!
-- SUPER flexibiln√≠ pro enterprise
```

**Obsidian:**
- `Learn/08-Data-Vault.md`
- Hub-Link-Sat diagram, v√Ωhody

**GitHub Task:**
```bash
git add projects/01-etl-local/data_vault_model.sql
git commit -m "üè¢ Implementing Data Vault Hub-Link-Satellite"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Lakehouse!

---

#### T√Ωden 9: Lakehouse Fundamenty - Bronze, Silver, Gold

**Hlavn√≠ t√©mata:**
- Lakehouse = DL + DW v jedn√© platformƒõ
- Medallion Architecture: Bronze ‚Üí Silver ‚Üí Gold
- Delta Lake form√°t
- Unity Catalog pro governance

**Uƒçebn√≠ c√≠le:**
- [ ] Rozum√≠m Medallion Architecture
- [ ] V√≠m, co pat≈ô√≠ do Bronze/Silver/Gold
- [ ] Zn√°m v√Ωhody Lakehouse
- [ ] V√≠m, jak propojit s dimensional modeling

**Praktika:**
```python
# Medallion Architecture v Databricks

# BRONZE: Raw data - "as-is" z source
# - API response ‚Üí JSON v parquet
# - Database export ‚Üí CSV v parquet
# - Streaming data ‚Üí Delta
# Transformace: Jen conversions (JSON‚Üítabular, dedup)

bronze_orders = spark.read.parquet("s3://bucket/bronze/orders/")
# Obsah: raw_data BLOB, ingestion_date, source_system

# SILVER: Enterprise view - ƒçi≈°tƒõn√≠, konformace
# - Duplik√°ty odstranƒõny
# - Null valuesolved
# - Data types konvertov√°ny
# - Business rules aplikov√°ny
# - –ú–û–ñ–ï–¢ b√Ωt zaƒç√°tek dim modelingu!

silver_orders = (
    bronze_orders
    .dropDuplicates(["order_id"])
    .filter("order_id IS NOT NULL")
    .withColumn("order_date", F.col("raw_date").cast("date"))
    .drop("raw_data", "ingestion_date")
)

# GOLD: Presentation layer - Business ready
# - Star Schema ƒçi Dimensional Model
# - Optimalizov√°no pro BI dotazy
# - Lze ƒç√≠st bezpeƒçnƒõ (sem maj√≠ p≈ô√≠stup analytici)

gold_fact_sales = (
    silver_orders
    .join(silver_customers, "customer_id")
    .join(silver_products, "product_id")
    .select("order_id", "customer_id", "product_id", "order_date", "quantity", "revenue")
)

# Write to Gold:
gold_fact_sales.write.mode("overwrite") \
    .option("mergeSchema", "true") \
    .format("delta") \
    .saveAsTable("gold.fact_sales")
```

**Obsidian:**
- `Learn/09-Lakehouse-Medallion.md`
- Bronze/Silver/Gold workflow, Python code

**GitHub Task:**
```bash
git add projects/01-etl-local/medallion_architecture.py
git commit -m "üè™ Medallion Architecture: Bronze ‚Üí Silver ‚Üí Gold"
```

**P≈ô√≠≈°t√≠ f√°ze:** Orchestrace a pipeline!

---

### F√ÅZE 4: PIPELINE, WORKFLOW, ORCHESTRACE (T√Ωdny 10-11)

#### T√Ωden 10: ETL/ELT Orchestrace s Airflow a Databricks

**Hlavn√≠ t√©mata:**
- Airflow DAGs pro orchestraci
- Databricks Workflows (native)
- Batch vs Streaming vs Micro-batch
- Idempotentn√≠ a inkrement√°ln√≠ z√°tƒõ≈æ

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m Airflow DAG patterns
- [ ] Rozum√≠m Watermarks (inkrement√°ln√≠ z√°tƒõ≈æi)
- [ ] V√≠m, jak dƒõlat MERGE (upsert)
- [ ] Zn√°m batch vs streaming trade-offs

**Praktika:**
```python
# Airflow DAG: Bronze ‚Üí Silver ‚Üí Gold Pipeline

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-engineering',
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'etl_lakehouse_pipeline',
    default_args=default_args,
    description='Medallion ETL: Bronze ‚Üí Silver ‚Üí Gold',
    schedule_interval='@daily',
    start_date=datetime(2025, 11, 1)
)

def extract_to_bronze():
    """Load raw data from source to Bronze"""
    # API/Database ‚Üí S3 Bronze layer (parquet)
    pass

def transform_to_silver():
    """Clean and validate data in Silver"""
    # Bronze ‚Üí Silver (dedup, null handling, type casting)
    pass

def build_gold_layer():
    """Create dimensional model in Gold"""
    # Silver ‚Üí Gold (Star Schema, fact + dimensions)
    pass

def publish_to_bi():
    """Publish to BI tools"""
    # Gold ‚Üí PowerBI / Tableau refresh
    pass

# Define tasks
t1 = PythonOperator(task_id='extract_bronze', python_callable=extract_to_bronze, dag=dag)
t2 = PythonOperator(task_id='transform_silver', python_callable=transform_to_silver, dag=dag)
t3 = PythonOperator(task_id='build_gold', python_callable=build_gold_layer, dag=dag)
t4 = PythonOperator(task_id='publish_bi', python_callable=publish_to_bi, dag=dag)

# Define dependencies
t1 >> t2 >> t3 >> t4
```

```python
# Inkrement√°ln√≠ z√°tƒõ≈æ s Watermarks (SQL)
# "Posledn√≠ synchronizace: 2025-11-01 10:00"

def incremental_load():
    """Naƒçti jen nov√° data od posledn√≠ synchronizace"""
    
    # ƒåti watermark
    last_sync = spark.sql("SELECT MAX(load_date) FROM silver.load_watermark").collect()[0][0]
    
    # ƒåti nov√° data
    new_data = spark.sql(f"""
        SELECT * FROM bronze.orders 
        WHERE ingestion_date > '{last_sync}'
    """)
    
    # MERGE do Silver (upsert - insert if new, update if changed)
    new_data.write.format("delta").mode("merge") \
        .option("mergeSchema", "true") \
        .saveAsTable("silver.orders")
    
    # Update watermark
    spark.sql(f"""
        INSERT INTO silver.load_watermark 
        VALUES ('{datetime.now()}')
    """)
```

**Obsidian:**
- `Learn/10-Pipeline-Orchestration.md`
- DAG patterns, watermarks

**GitHub Task:**
```bash
git add projects/02-cloud-pipeline/airflow_dag.py
git commit -m "üîÑ ETL/ELT orchestration with Airflow and incremental loads"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Datov√° kvalita a governance!

---

#### T√Ωden 11: Data Quality, Governance, Unity Catalog

**Hlavn√≠ t√©mata:**
- Data Quality checks (EXPECT, DROP, FAIL)
- PK/FK constraints (informativn√≠)
- Identity Columns pro Surrogate Keys
- Unity Catalog pro centralizovanou spr√°vu

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m DQ check patterns
- [ ] V√≠m, jak implementovat constraints
- [ ] Rozum√≠m Unity Catalog
- [ ] Zn√°m Environment-aware ACLs

**Praktika:**
```python
# Data Quality v Databricks

from databricks.sdk.runtime import sql

# Constraint 1: NOT NULL
spark.sql("""
    ALTER TABLE silver.orders 
    ADD CONSTRAINT order_id_not_null 
    CHECK (order_id IS NOT NULL)
""")

# Constraint 2: Surrogate key
spark.sql("""
    CREATE TABLE gold.dim_customer (
        customer_key INT GENERATED BY DEFAULT AS IDENTITY,
        customer_id INT NOT NULL,
        name VARCHAR(100) NOT NULL,
        country VARCHAR(50)
    )
""")

# Data Quality Checks (Delta Live Tables)
import dlt

@dlt.table(quality='silver')
def orders_with_quality():
    return spark.sql("""
        SELECT 
            order_id,
            customer_id,
            order_date,
            amount
        FROM bronze.orders
        WHERE order_id IS NOT NULL
            AND amount > 0
            AND order_date BETWEEN '2020-01-01' AND CURRENT_DATE
    """)

# Unity Catalog: Centralizovan√° spr√°va

# Level 1: CATALOG (prost≈ôed√≠: prod, staging, dev)
# Level 2: SCHEMA (business area: finance, marketing, sales)
# Level 3: TABLE/VIEW (konkr√©tn√≠ objekt)

# P≈ô√≠klad:
# prod.finance.fact_revenue
# staging.marketing.dim_campaign
# dev.sales.stg_orders

# ACL: Kdo m√° p≈ô√≠stup?
spark.sql("""
    GRANT SELECT ON SCHEMA prod.finance TO finance_team
    GRANT MODIFY ON TABLE prod.finance.fact_revenue TO finance_admin
    GRANT READ_METADATA ON CATALOG prod TO all_users
""")

# Lineage tracking (automaticky!)
# Databricks vid√≠: bronze.orders ‚Üí silver.orders ‚Üí gold.fact_sales
```

**Obsidian:**
- `Learn/11-Data-Quality-Governance.md`
- DQ patterns, constraints, ACLs

**GitHub Task:**
```bash
git add projects/02-cloud-pipeline/data_quality_checks.py
git commit -m "‚úÖ Data Quality checks and Unity Catalog governance"
```

**N√°sleduj√≠c√≠ f√°ze:** Cloud platforms!

---

### F√ÅZE 5: CLOUD PLATFORMY A INTEGRACE (T√Ωdny 12-13)

#### T√Ωden 12: Cloud Platforms - AWS, Azure, GCP

**Hlavn√≠ t√©mata:**
- AWS (S3, Redshift, Glue, EMR)
- Azure (Blob, Synapse, Data Factory)
- GCP (Storage, BigQuery, Dataflow)
- Databricks na v≈°ech cloud platform√°ch

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m cloud services pro data eng
- [ ] V√≠m, jak zvolit cloud platformu
- [ ] Rozum√≠m hybridn√≠m p≈ô√≠stup≈Øm
- [ ] Zn√°m Databricks availability

**Praktika:**
```python
# AWS: S3 + Glue + Redshift

# 1. Data v S3 (Bronze layer)
s3_path = "s3://my-bucket/bronze/orders/"

# 2. Glue Crawler: Automatick√© schema discovery
# AWS Glue ‚Üí crawls S3 ‚Üí detects schema ‚Üí Glue Catalog

# 3. SQL v Athena (query S3 p≈ô√≠mo bez ETL)
spark.sql("""
    SELECT order_id, customer_id, amount 
    FROM s3_bronze_orders 
    WHERE order_date = '2025-11-01'
""")

# 4. Redshift: DW pro analytics
# UNLOAD data do Redshift pro BI
```

```python
# Azure: Blob Storage + Synapse + Data Factory

# 1. Data v Azure Blob (Bronze)
blob_path = "wasbs://container@account.blob.core.windows.net/bronze/"

# 2. Azure Synapse: SQL DW (like Redshift)
# Dedicated SQL Pool pro data warehouse

# 3. Azure Data Factory: Orchestration (like Airflow)
# Visual ETL builder
```

```python
# GCP: BigQuery (Serverless DW!) + Cloud Storage + Dataflow

# 1. Data v Cloud Storage (Bronze)
gcs_path = "gs://my-bucket/bronze/orders/"

# 2. BigQuery: SQL DW bez provisioning!
spark.sql("""
    SELECT order_id, SUM(amount) 
    FROM `project.dataset.fact_sales` 
    GROUP BY order_id
""")

# 3. Dataflow: Stream processing (like Spark Streaming)
```

**Obsidian:**
- `Learn/12-Cloud-Platforms.md`
- AWS vs Azure vs GCP porovn√°n√≠

**GitHub Task:**
```bash
git add docs/cloud-platforms-comparison.md
git commit -m "‚òÅÔ∏è Cloud platforms: AWS, Azure, GCP comparison"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Integrace + dbt!

---

#### T√Ωden 13: Integrace (Fivetran, dbt Labs) a Transformace

**Hlavn√≠ t√©mata:**
- Fivetran: ELT (Extract-Load-Transform)
- dbt: SQL transformace, testing, docs
- CI/CD s dbt
- Change Data Capture (CDC)

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m Fivetran workflow (ELT pattern)
- [ ] Um√≠m ps√°t dbt modely
- [ ] V√≠m, jak dƒõlat dbt testing
- [ ] Rozum√≠m CDC pro real-time

**Praktika:**
```yaml
# Fivetran: Deklarativn√≠ ETL
# Config v YAML - v≈°e se synchronizuje!

connector: salesforce
destination: snowflake
sync_frequency: 1440  # ka≈æd√Ωch 24h

schema_settings:
  Account:
    - Id
    - Name
    - Revenue
  Opportunity:
    - Id
    - AccountId
    - Amount
    - StageName
    - CloseDate

# Fivetran automatic:
# - Crawluje schema ze Salesforce
# - Pushuje do Snowflake (DBT format)
# - Dƒõl√° SCD Type 2 automaticky!
```

```sql
-- dbt: SQL transformace s verz√≠ kontorem

-- models/staging/stg_orders.sql
SELECT 
    order_id,
    customer_id,
    CAST(order_date AS DATE) as order_date,
    ROUND(quantity * unit_price, 2) as revenue
FROM {{ source('raw', 'orders') }}
WHERE order_id IS NOT NULL

-- tests/stg_orders_tests.yml
models:
  - name: stg_orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: revenue
        tests:
          - assert_positive_revenue

-- models/marts/fact_sales.sql
SELECT 
    {{ dbt_utils.surrogate_key(['order_id', 'order_date']) }} as sales_key,
    order_id,
    customer_id,
    order_date,
    revenue
FROM {{ ref('stg_orders') }}

-- dbt run ‚Üí builds all models
-- dbt test ‚Üí runs all tests
-- dbt docs generate ‚Üí auto documentation!
```

**Obsidian:**
- `Learn/13-Fivetran-dbt-Integration.md`
- ELT vs ETL, dbt workflow

**GitHub Task:**
```bash
git add projects/02-cloud-pipeline/dbt_project/
git commit -m "üèóÔ∏è dbt transformations with testing and documentation"
```

**N√°sleduj√≠c√≠ f√°ze:** BI, AI, a pokroƒçil√© optimalizace!

---

### F√ÅZE 6: BI, AI, A POKROƒåIL√â OPTIMALIZACE (T√Ωdny 14-17)

#### T√Ωden 14: BI, Natural Language, a DatabricksIQ

**Hlavn√≠ t√©mata:**
- BI reporting: Power BI, Tableau, Databricks SQL
- Natural Language Queries
- DatabricksIQ: AI pro semantic layer
- AI Functions (sentiment analysis v SQL)

**Uƒçebn√≠ c√≠le:**
- [ ] Um√≠m ps√°t BI dotazy
- [ ] V√≠m, jak queryovat p≈ôirozen√Ωm jazykem
- [ ] Rozum√≠m DatabricksIQ
- [ ] Zn√°m AI functions

**Praktika:**
```python
# Natural Language Query v Databricks SQL

# Norm√°ln√≠ SQL:
SELECT 
    DATE_TRUNC('MONTH', order_date) as month,
    SUM(revenue) as total_revenue,
    COUNT(*) as order_count
FROM gold.fact_sales
WHERE order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 12 MONTH)
GROUP BY 1
ORDER BY 1 DESC;

# P≈ôirozen√Ω jazyk (DatabricksIQ):
# "Show me monthly revenue trend for past year"
# ‚Üí AI p≈ôelo≈æ√≠ na SQL v√Ω≈°e! ü§Ø

# AI Functions: Sentiment analysis
SELECT 
    customer_id,
    feedback_text,
    AI.SENTIMENT_ANALYSIS(feedback_text) as sentiment_score,
    CASE 
        WHEN sentiment_score > 0.7 THEN 'Positive'
        WHEN sentiment_score < 0.3 THEN 'Negative'
        ELSE 'Neutral'
    END as sentiment
FROM gold.customer_feedback;
```

**Obsidian:**
- `Learn/14-BI-Natural-Language.md`
- NLQ examples, DatabricksIQ

**GitHub Task:**
```bash
git add projects/02-cloud-pipeline/bi_queries.sql
git commit -m "üìä BI queries and Natural Language support"
```

**P≈ô√≠≈°t√≠ t√Ωden:** ML operations a RAG!

---

#### T√Ωden 15: MLflow, RAG, a AI Operations

**Hlavn√≠ t√©mata:**
- MLflow: Experiment tracking, model registry
- RAG (Retrieval Augmented Generation) s daty
- Feature stores
- Model deployment

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m MLflow workflow
- [ ] Um√≠m buildovat RAG pipeline
- [ ] V√≠m, jak deployovat ML modely
- [ ] Rozum√≠m feature stores

**Praktika:**
```python
# MLflow: Experiment tracking

import mlflow
from sklearn.ensemble import RandomForestRegressor

mlflow.start_run()

# Hyperparameters
n_estimators = 100
max_depth = 10

# Train model
model = RandomForestRegressor(
    n_estimators=n_estimators,
    max_depth=max_depth
)
model.fit(X_train, y_train)

# Log metrics
score = model.score(X_test, y_test)
mlflow.log_metric("r2_score", score)
mlflow.log_param("n_estimators", n_estimators)
mlflow.log_param("max_depth", max_depth)

# Log model
mlflow.sklearn.log_model(model, "random_forest_model")

mlflow.end_run()

# V≈°echny experimenty se trackuj√≠! (UI pro comparison)
```

```python
# RAG Pipeline: Retrieval + Generation

# 1. Data z Gold layer ‚Üí embeddings
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Load your company data
documents = spark.sql("SELECT * FROM gold.company_knowledge_base").toPandas()

# Embed a store v vector DB
embeddings = OpenAIEmbeddings()
vectordb = Chroma.from_documents(documents, embeddings)

# 2. User query ‚Üí retrieve relevant docs
query = "What is our Q3 revenue?"
relevant_docs = vectordb.similarity_search(query, k=3)

# 3. LLM s context ‚Üí generate answer
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="Based on {context}, answer: {question}"
)

llm = ChatOpenAI(model="gpt-4")
answer = llm.predict(
    text=prompt.format(
        context=relevant_docs,
        question=query
    )
)

# Result: AI-generated answer based on YOUR data! (not hallucinations!)
```

**Obsidian:**
- `Learn/15-MLflow-RAG.md`
- Experiment tracking, RAG workflow

**GitHub Task:**
```bash
git add projects/03-realtime-rag/mlflow_tracking.py
git commit -m "ü§ñ MLflow experiments and RAG pipeline"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Optimalizace a streaming!

---

#### T√Ωden 16: Pokroƒçil√© Optimalizace - Z-Order, Photon, Serverless

**Hlavn√≠ t√©mata:**
- Z-Order indexing (clustered index)
- Photon Engine (MPP query engine)
- Predictive I/O (AI-driven optimization)
- Serverless compute (autoscaling)

**Uƒçebn√≠ c√≠le:**
- [ ] V√≠m, jak optimalizovat queries
- [ ] Zn√°m Z-Order pattern
- [ ] Rozum√≠m Photon Engine
- [ ] V√≠m, jak nastavit auto-scaling

**Praktika:**
```python
# Z-Order indexing: "Smart" clustering

# Norm√°ln√≠: data jsou n√°hodnƒõ distribuov√°ny
# Z-Order: data jsou clustered podle sloupc≈Ø

spark.sql("""
    OPTIMIZE gold.fact_sales
    USING Z_ORDER BY (customer_id, order_date)
""")

# Teƒè query:
# "Dej mi objedn√°vky z√°kazn√≠ka 123 za 2025"
# Jen prohled√° "mal√Ω" cluster m√≠sto cel√© tabulky! ‚ö°

# V√Ωsledek: Queries jsou 10-100x rychlej≈°√≠!
```

```python
# Predictive I/O: Databricks AI autopilot

# Databricks vid√≠ tv√© dotazy a automaticky:
# - Clustere data
# - Optimalizuje v√Ωkon
# - Bez tv√©ho z√°sahu!

# Filestore v Python:
optimized_tables = spark.sql("""
    SELECT * FROM system.statistics.table_stats
    WHERE optimization_opportunity > 0.5  -- 50% speedup mo≈æn√Ω!
""")
```

**Obsidian:**
- `Learn/16-Optimization.md`
- Z-Order, Photon, VACUUM patterns

**GitHub Task:**
```bash
git add projects/02-cloud-pipeline/optimization.sql
git commit -m "‚ö° Z-Order indexing and query optimization"
```

**P≈ô√≠≈°t√≠ t√Ωden:** Streaming & Real-time!

---

#### T√Ωden 17: Streaming Data - Micro-batch, Auto Loader, Real-time ETL

**Hlavn√≠ t√©mata:**
- Micro-batch vs True Streaming
- Auto Loader (cloud-native SFTP)
- Structured Streaming v Spark
- Real-time fact tables

**Uƒçebn√≠ c√≠le:**
- [ ] Zn√°m streaming architectury
- [ ] Um√≠m ps√°t Spark Streaming
- [ ] V√≠m, jak deployovat real-time ETL
- [ ] Rozum√≠m Event Time vs Processing Time

**Praktika:**
```python
# Auto Loader: Kontinu√°ln√≠ ingest z cloud storage

df = spark.readStream \
    .format("cloudFiles") \
    .option("cloudFiles.format", "parquet") \
    .option("cloudFiles.schemaLocation", "/checkpoints/schema") \
    .load("/mnt/streaming-source/")

df.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints/stream") \
    .table("bronze.streaming_events")

# Auto Loader automaticky:
# - Detekuje nov√© soubory
# - Ingestuje je
# - Trackuje progres (checkpoint)
```

```python
# Structured Streaming: Real-time transformace

events = spark.readStream.table("bronze.streaming_events")

# Group by v real-time
summary = events \
    .groupBy(
        F.window("timestamp", "5 minutes"),  # 5-minute windows
        "event_type"
    ) \
    .agg(
        F.count("*").alias("count"),
        F.avg("value").alias("avg_value")
    )

# Write real-time results
summary.writeStream \
    .format("delta") \
    .outputMode("update") \
    .option("checkpointLocation", "/checkpoints/summary") \
    .table("silver.event_summary")

# V√Ωsledek: Ka≈æd√Ωch 5 minut = nov√Ω update v tabulce!
```

**Obsidian:**
- `Learn/17-Streaming-Real-time.md`
- Micro-batch, Auto Loader, windowing

**GitHub Task:**
```bash
git add projects/03-realtime-rag/streaming_pipeline.py
git commit -m "üåä Real-time streaming with Spark Structured Streaming"
```

**N√°sleduj√≠c√≠ f√°ze:** Case studies a career!

---

### F√ÅZE 7: CASE STUDIES, PRAKTIKA A KARI√âRA (T√Ωdny 18-22)

#### T√Ωden 18: Case Study - Airbnb Data Platform

**T√©ma:** Jak Airbnb builduje sv≈Øj data platform?

**Uƒçebn√≠ body:**
- Airbnb m√° **50+ PB** dat roƒçnƒõ
- Real-time bookings analytics
- Marketplace pricing engine
- Quality assurance pro hosts

**Architektura:**
```
Events (JS, app, server) 
  ‚Üì Kafka (streaming)
  ‚Üì
Hadoop Ecosystem (HDFS, Spark, Hive)
  ‚Üì
Real-time: Kafka ‚Üí Spark Streaming ‚Üí HBase
Batch: HDFS ‚Üí Spark SQL ‚Üí Hive
  ‚Üì
BI: Tableau, internal tools
ML: Feature stores, models
```

**Tv≈Øj z√°pis:**
- [ ] Jak√© jsou hlavn√≠ data sources?
- [ ] Proƒç Hadoop? Teƒè by se mo≈æn√° volili Databricks...
- [ ] Jak dƒõlaj√≠ real-time (Kafka)?
- [ ] Jakou architekturu bys zvolil dnes?

**Obsidian Task:**
```
Learn/18-Case-Study-Airbnb.md
- Architecture diagram
- Key learnings
- Modern alternatives
```

---

#### T√Ωden 19: Case Study - Uber Data & Analytics

**T√©ma:** Jak Uber pou≈æ√≠v√° data pro real-time dispatch?

**Uƒçebn√≠ body:**
- Uber generuje **13+ milion≈Ø** trip≈Ø dennƒõ
- Real-time dispatch engine
- Surge pricing
- Driver ratings, customer behavior

**Architektura:**
```
Ride requests (app)
  ‚Üì Real-time processing
  ‚Üì
Kafka ‚Üí Flink/Spark Streaming ‚Üí Feature stores
  ‚Üì
ML Model: Price optimization
  ‚Üì
Decision: Assign driver, set price, route
```

**Tv≈Øj z√°pis:**
- [ ] Jak se processuj√≠ miliony event≈Ø?
- [ ] Latency requirements?
- [ ] Jak se integruje ML do real-time syst√©mu?
- [ ] Data retention policies?

**Obsidian Task:**
```
Learn/19-Case-Study-Uber.md
- Real-time requirements
- ML integration
- Pricing algorithm
```

---

#### T√Ωden 20: Praktick√° Cviƒçen√≠ - Build End-to-End Pipeline

**Projekt:** Tv≈Øj vlastn√≠ Lakehouse pipeline

**Specifikace:**
```
1. Data ingestion: API ‚Üí Bronze (Python + requests)
2. Transformation: Bronze ‚Üí Silver (SQL, dbt)
3. Dimensional modeling: Silver ‚Üí Gold (Star Schema)
4. BI: Queries v DatabricksSQL
5. Monitoring: Data quality checks
```

**P≈ô√≠klad:** Stock market data pipeline
```
API: Alpha Vantage (stock prices)
  ‚Üì
Bronze: RAW ticker data (parquet)
  ‚Üì
Silver: Cleaned, typed, deduplicated
  ‚Üì
Gold: dim_stock, fact_price (Star Schema)
  ‚Üì
BI: Daily price trends, moving averages
```

**Deliverable:**
- Python ingestion script
- SQL transformations (dbt)
- Star Schema DDL
- 5x BI queries
- Data quality checks
- README s architecture

**GitHub Task:**
```bash
git add projects/03-realtime-rag/
git commit -m "üéØ End-to-end Lakehouse pipeline: ingestion ‚Üí transformation ‚Üí BI"
```

---

#### T√Ωden 21: Interview Prep - Behavioral & Technical

**Technick√© ot√°zky:**
```
1. "Design a data pipeline for 1 billion events/day"
   - Odpovƒõƒè: Medallion arch, auto-loader, Delta, Databricks
   
2. "Dimenzion√°ln√≠ model pro e-commerce"
   - fact_orders, dim_date, dim_customer, dim_product
   - SCD Type 2 pro products
   
3. "Jak optimalizujete slow query?"
   - EXPLAIN PLAN, Z-Order, VACUUM, partitioning
   
4. "Star vs Snowflake Schema?"
   - Star: denormalizovan√°, rychl√°, redundance
   - Snowflake: normalizovan√°, modul√°rn√≠, v√≠ce join≈Ø
```

**Behavioral:**
```
1. "Tell me about your biggest challenge"
2. "How do you handle pressure?"
3. "Give an example of collaboration"
4. "Biggest mistake? What did you learn?"
```

**Obsidian:**
```
Docs/Interview-Prep.md
- Technical Q&A
- System design templates
- Behavioral frameworks
```

---

#### T√Ωden 22: Shrnut√≠, Certifikace a Career Plan

**Co jste se nauƒçili:**
- ‚úÖ Datov√© modelov√°n√≠ (Star, Snowflake, Vault)
- ‚úÖ Lakehouse architecture (Bronze-Silver-Gold)
- ‚úÖ ETL/ELT orchestrace (Airflow)
- ‚úÖ Data quality a governance
- ‚úÖ Real-time streaming
- ‚úÖ BI a AI integration

**Portfolio na GitHub:**
- ‚úÖ 5+ projekt≈Ø
- ‚úÖ 300+ commit≈Ø
- ‚úÖ Komplexn√≠ architektura
- ‚úÖ Professional README

**Career Options:**
```
Option 1: Junior Data Engineer (55-70k CZK)
- Budete dƒõlat ETL/ELT pipelines
- Maintenance existuj√≠c√≠ch model≈Ø
- Data quality monitoring

Option 2: Analytics Engineer (60-75k CZK)
- dbt focus
- SQL optimization
- BI ad-hoc reporty

Option 3: Python/Data Platform Engineer (70-85k CZK)
- More coding (Python/Scala)
- Infrastructure
- Performance optimization

Option 4: Specialization (6-12 mƒõs√≠c≈Ø)
- Streaming Engineer (Kafka, Flink)
- ML Engineer (feature stores, models)
- Data Architect (design systems)
```

**Certifikace doporuƒçen√°:**
- Databricks Certified Data Engineer
- AWS Certified Data Analytics
- dbt Fundamentals

**LinkedIn Profile:**
- 300+ connections
- 20+ posts o learningi
- 5 case studies
- 10+ recommendations (douf√°me!)

---

## üìä POROVN√ÅN√ç: 23-T√ùDENN√ç vs 22-T√ùDENN√ç PL√ÅN

| Aspekt | Origin√°ln√≠ (23 t√Ωdn≈Ø) | Pokroƒçil√Ω (22 t√Ωdn≈Ø) |
|--------|----------------------|----------------------|
| **Fundamenty** | T√Ωdny 1-8 | T√Ωdny 1-2 |
| **Python & SQL** | T√Ωdny 6-9 | Integrov√°no |
| **Cloud (GCP)** | T√Ωdny 11-16 | T√Ωdny 10-13 (AWS/Azure/GCP) |
| **Modelov√°n√≠** | Z√°klady v 16 | Hluboko v 3-9 |
| **Lakehouse** | Jenom zm√≠nka | F√°ze 3-4 |
| **Real-time** | Project 3 | T√Ωden 17 |
| **AI/ML** | Bonus | F√°ze 6 |
| **Career** | Zm√≠nka | T√Ωden 22 |

**Zvolte si:**
- **Origin√°ln√≠**: –ï—Å–ª–∏ chcete z√°klady + time na pr√°ci
- **Pokroƒçil√Ω**: Pokud chcete enterprise-ready znalosti

---

## ‚úÖ CHECKLIST: Co m√°te?

- [x] 22-t√Ωdenn√≠ pl√°n
- [x] Vƒõdomosti: Star/Snowflake/Vault
- [x] Lakehouse Medallion Architecture
- [x] ETL/ELT orchestrace
- [x] Real-time streaming
- [x] BI & AI integration
- [x] Cloud comparison
- [x] Case studies
- [x] Interview prep
- [x] Career guidance

**Teƒè jen IMPLEMENTUJTE!** üöÄ

---

**Posledn√≠ slova:**
Tento pl√°n je va≈°i "zku≈°ebn√≠ sk√°la" k datov√©mu in≈æen√Ωrstv√≠. Nen√≠ teoretick√Ω - je praktick√Ω, foundation-first, a enterprise-ready. Do b≈ôezna 2026 budete m√≠t skills, kter√© si IT firmy velmi ce≈àuj√≠.

**Zaƒçnƒõte DNES. Commitujte DENNƒö. Na b≈ôezen budete READY!** üí™
